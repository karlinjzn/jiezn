{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from time import time\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import math\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "from zipfile import ZipFile\n",
    "from scipy.cluster.vq import *\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "from skimage.feature import daisy,hog\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (299, 299, 3)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRECTORY = \"MantaRay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each class to an integer label\n",
    "class_mapping = {}\n",
    "index = 0\n",
    "class_images = {}\n",
    "# Create dictionary to map integer labels to individuals\n",
    "# Class_images will record number of images for each class\n",
    "for directory in glob.glob(DATASET_DIRECTORY + '/*'):\n",
    "    class_mapping[index] = directory.split(\"/\")[1]\n",
    "    class_images[index] = len(glob.glob(directory + '/*_CR.jpg')) + len(glob.glob(directory + '/*_CR.JPG'))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720,) (720,)\n"
     ]
    }
   ],
   "source": [
    "image_arrays = []\n",
    "image_labels = []\n",
    "image_paths = []\n",
    "\n",
    "for label, individual in class_mapping.items():\n",
    "    for directory in glob.glob(DATASET_DIRECTORY + '/*'):\n",
    "        if directory.split(\"/\")[1] == individual:\n",
    "            image_directory = directory\n",
    "            break\n",
    " \n",
    "    for image in glob.glob(directory + '/*_EH.jpg'):\n",
    "        image_paths.append(image)\n",
    "        image = plt.imread(image)\n",
    "        image_arrays.append(image)\n",
    "        image_labels.append(label)\n",
    "        \n",
    "    for image in glob.glob(directory + '/*_EH.JPG'):\n",
    "        image_paths.append(image)\n",
    "        image = plt.imread(image)\n",
    "        image_arrays.append(image)\n",
    "        image_labels.append(label)\n",
    "        \n",
    "image_arrays = np.array(image_arrays)\n",
    "image_labels = np.array(image_labels)\n",
    "print(image_arrays.shape, image_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data augmentation for SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "for n in range(len(image_paths)):\n",
    "    img = load_img(image_paths[n])  # this is a PIL image, please replace to your own file path\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "    # the .flow() command below generates batches of randomly transformed images\n",
    "    # and saves the results to the `preview/` directory\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, \n",
    "                              batch_size=1,\n",
    "                              save_to_dir='MantaRay/' + image_paths[n].split('/')[1],  \n",
    "                              save_prefix= 'AUG_' + image_paths[n].split('/')[2], \n",
    "                              save_format='jpg'):\n",
    "        i += 1\n",
    "        if i > 2:\n",
    "            break  # otherwise the generator would loop indefinitely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. SURF & SIFT extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720,) (720,)\n"
     ]
    }
   ],
   "source": [
    "image_arrays = []\n",
    "image_labels = []\n",
    "image_paths = []\n",
    "#root_image_directory = \"images/faces/\"\n",
    "for label, individual in class_mapping.items():\n",
    "    for directory in glob.glob(DATASET_DIRECTORY + '/*'):\n",
    "        if directory.split(\"/\")[1] == individual:\n",
    "            image_directory = directory\n",
    "            break\n",
    " \n",
    "    for image in glob.glob(directory + '/*_EH.jpg'):\n",
    "        image_paths.append(image)\n",
    "        image = plt.imread(image)\n",
    "        image_arrays.append(image)\n",
    "        image_labels.append(label)\n",
    "        \n",
    "    for image in glob.glob(directory + '/*_EH.JPG'):\n",
    "        image_paths.append(image)\n",
    "        image = plt.imread(image)\n",
    "        image_arrays.append(image)\n",
    "        image_labels.append(label)\n",
    "        \n",
    "image_arrays = np.array(image_arrays)\n",
    "image_labels = np.array(image_labels)\n",
    "print(image_arrays.shape, image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160,) (2160,)\n"
     ]
    }
   ],
   "source": [
    "aug_image_arrays = []\n",
    "aug_image_labels = []\n",
    "aug_image_paths = []\n",
    "#root_image_directory = \"images/faces/\"\n",
    "for label, individual in class_mapping.items():\n",
    "    for directory in glob.glob(DATASET_DIRECTORY + '/*'):\n",
    "        if directory.split(\"/\")[1] == individual:\n",
    "            image_directory = directory\n",
    "            break\n",
    " \n",
    "    for image in glob.glob(directory + '/AUG_*.jpg'):\n",
    "        aug_image_paths.append(image)\n",
    "        image = plt.imread(image)\n",
    "        aug_image_arrays.append(image)\n",
    "        aug_image_labels.append(label)\n",
    "\n",
    "aug_image_arrays = np.array(aug_image_arrays)\n",
    "aug_image_labels = np.array(aug_image_labels)\n",
    "print(aug_image_arrays.shape, aug_image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_keypoints(keypoints, descriptors):\n",
    "    i = 0\n",
    "    temp_array = []\n",
    "    for point in keypoints:\n",
    "        temp = (point.pt, point.size, point.angle, point.response, point.octave,\n",
    "        point.class_id, descriptors[i])     \n",
    "        ++i\n",
    "        temp_array.append(temp)\n",
    "    return temp_array\n",
    "\n",
    "def unpickle_keypoints(array):\n",
    "    keypoints = []\n",
    "    descriptors = []\n",
    "    for point in array:\n",
    "        temp_feature = cv2.KeyPoint(x=point[0][0],y=point[0][1],_size=point[1], _angle=point[2], _response=point[3], _octave=point[4], _class_id=point[5])\n",
    "        temp_descriptor = point[6]\n",
    "        keypoints.append(temp_feature)\n",
    "        descriptors.append(temp_descriptor)\n",
    "    return keypoints, np.array(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "\n",
    "for i in range(len(image_paths)):\n",
    "    # Load the images\n",
    "    img1 =cv2.imread(image_paths[i])\n",
    "\n",
    "    # Convert them to greyscale\n",
    "    grey_img1 =cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # SURF extraction\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    kp1, desc1 = surf.detectAndCompute(grey_img1,None)\n",
    "\n",
    "    #Store and Retrieve keypoint features\n",
    "    temp_array = []\n",
    "    temp = pickle_keypoints(kp1, desc1)\n",
    "    temp_array.append(temp)\n",
    "\n",
    "    pickle.dump(temp_array, open('MantaRay/'+image_paths[i].split('/')[1]+'/'+image_paths[i].split('/')[2].split('_')[0]+\"_SURF.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(image_paths)):\n",
    "    # Load the images\n",
    "    img1 =cv2.imread(image_paths[i])\n",
    "\n",
    "    # Convert them to greyscale\n",
    "    grey_img1 =cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # SIFT extraction\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp1, desc1 = sift.detectAndCompute(grey_img1,None)\n",
    "\n",
    "    #Store and Retrieve keypoint features\n",
    "    temp_array = []\n",
    "    temp = pickle_keypoints(kp1, desc1)\n",
    "    temp_array.append(temp)\n",
    "\n",
    "    pickle.dump(temp_array, open('MantaRay/'+image_paths[i].split('/')[1]+'/'+image_paths[i].split('/')[2].split('_')[0]+\"_SIFT.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(aug_image_paths)):\n",
    "    # Load the images\n",
    "    img1 =cv2.imread(aug_image_paths[i])\n",
    "\n",
    "    # Convert them to greyscale\n",
    "    grey_img1 =cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # SURF extraction\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    kp1, desc1 = surf.detectAndCompute(grey_img1,None)\n",
    "\n",
    "    #Store and Retrieve keypoint features\n",
    "    temp_array = []\n",
    "    temp = pickle_keypoints(kp1, desc1)\n",
    "    temp_array.append(temp)\n",
    "\n",
    "    pickle.dump(temp_array, open('MantaRay/'+aug_image_paths[i].split('/')[1]+'/'+aug_image_paths[i].split('/')[2]+\"_SURF.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(aug_image_paths)):\n",
    "    # Load the images\n",
    "    img1 =cv2.imread(aug_image_paths[i])\n",
    "\n",
    "    # Convert them to greyscale\n",
    "    grey_img1 =cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # SURF extraction\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp1, desc1 = sift.detectAndCompute(grey_img1,None)\n",
    "\n",
    "    #Store and Retrieve keypoint features\n",
    "    temp_array = []\n",
    "    temp = pickle_keypoints(kp1, desc1)\n",
    "    temp_array.append(temp)\n",
    "\n",
    "    pickle.dump(temp_array, open('MantaRay/'+aug_image_paths[i].split('/')[1]+'/'+aug_image_paths[i].split('/')[2]+\"_SIFT.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve Keypoint Features\n",
    "keypoints_database = pickle.load(open(\"MantaRay/Four/Four 3_SURF.p\", \"rb\"))\n",
    "kp1, desc1 = unpickle_keypoints(keypoints_database[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. SIFT Matcher with 2160 aug, 720 aug, and no aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_paths = image_paths + aug_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = []\n",
    "for i in range(len(total_paths)):\n",
    "    total_labels.append(total_paths[i].split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-728-144bdce2ec12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# find the keypoints and descriptors with SIFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mkp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# FLANN parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matching_score = []\n",
    "\n",
    "for i in range(len(total_paths)-1):\n",
    "    print(i)\n",
    "    scores = []\n",
    "    img1 = cv2.imread(total_paths[i],0)     # queryImage\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    for j in range(i+1,len(total_paths)):\n",
    "        \n",
    "        img2 = cv2.imread(total_paths[j],0) # trainImage\n",
    "\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)\n",
    "        flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append([m])\n",
    "\n",
    "        scores.append(len(good)/np.min([len(des1),len(des2)]))\n",
    "        \n",
    "    matching_score.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full_aug_testset.txt', 'w') as f:\n",
    "    for item in total_paths[:190]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-729-d8881c53093a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# find the keypoints and descriptors with SIFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mkp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# FLANN parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "submatching_score = []\n",
    "\n",
    "for i in range(1439):\n",
    "    print(i)\n",
    "    scores = []\n",
    "    img1 = cv2.imread(total_paths[i],0)     # queryImage\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    for j in range(i+1,1440):\n",
    "        \n",
    "        img2 = cv2.imread(total_paths[j],0) # trainImage\n",
    "\n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "        \n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)   # or pass empty dictionary\n",
    "        flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "        \n",
    "        # Apply ratio test\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append([m])\n",
    "\n",
    "        scores.append(len(good)/np.min([len(des1),len(des2)]))\n",
    "        \n",
    "    submatching_score.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-768-946d3e40acac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# find the keypoints and descriptors with SIFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mkp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# FLANN parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(144,1439):\n",
    "    print(i)\n",
    "    scores = []\n",
    "    img1 = cv2.imread(total_paths[i],0)     # queryImage\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    for j in range(i+1,1440):\n",
    "        \n",
    "        img2 = cv2.imread(total_paths[j],0) # trainImage\n",
    "\n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "        \n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)   # or pass empty dictionary\n",
    "        flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "        \n",
    "        # Apply ratio test\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append([m])\n",
    "\n",
    "        scores.append(len(good)/np.min([len(des1),len(des2)]))\n",
    "        \n",
    "    submatching_score.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('half_aug_testset.txt', 'w') as f:\n",
    "    for item in total_paths[:208]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "for i in range(len(matching_score)):\n",
    "    score_tmp = matching_score[i]\n",
    "    score_tmp.insert(0,1) # matching with the individual itself\n",
    "    for j in reversed(range(i)):\n",
    "        score_tmp.insert(0,matching_score[j][i-j])\n",
    "    score_list.append(score_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating top-k accuracies for full matching (2880) images\n",
    "total_1 = 0\n",
    "total_5 = 0\n",
    "total_10 = 0\n",
    "total_25 = 0\n",
    "total_50 = 0\n",
    "total_75 = 0\n",
    "total_100 = 0\n",
    "true_1 = 0\n",
    "true_5 = 0\n",
    "true_10 = 0\n",
    "true_25 = 0\n",
    "true_50 = 0\n",
    "true_75 = 0\n",
    "true_100 = 0\n",
    "\n",
    "for i in range(len(score_list)):\n",
    "        true_label = total_labels[i]\n",
    "        top_1_idx = np.argsort(score_list[i])[-1:]\n",
    "        top_5_idx = np.argsort(score_list[i])[-5:]\n",
    "        top_10_idx = np.argsort(score_list[i])[-10:]\n",
    "        top_25_idx = np.argsort(score_list[i])[-25:]\n",
    "        top_50_idx = np.argsort(score_list[i])[-50:]\n",
    "        top_75_idx = np.argsort(score_list[i])[-75:]\n",
    "        top_100_idx = np.argsort(score_list[i])[-100:]\n",
    "        total_1 += 1\n",
    "        total_5 += 1\n",
    "        total_10 += 1\n",
    "        total_25 += 1\n",
    "        total_50 += 1\n",
    "        total_75 += 1\n",
    "        total_100 += 1\n",
    "        for index in top_1_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    #print(i)\n",
    "                    true_1 += 1\n",
    "                    break\n",
    "        for index in top_5_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_5 += 1\n",
    "                    break\n",
    "        for index in top_10_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_10 += 1\n",
    "                    break\n",
    "        for index in top_25_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_25 += 1\n",
    "                    break\n",
    "        for index in top_50_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_50 += 1\n",
    "                    break\n",
    "        for index in top_75_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_75 += 1\n",
    "                    break\n",
    "        for index in top_100_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_100 += 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18421052631578946\n",
      "0.6684210526315789\n",
      "0.8842105263157894\n",
      "0.9842105263157894\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(true_1/total_1)\n",
    "print(true_5/total_5)\n",
    "print(true_10/total_10)\n",
    "print(true_25/total_25)\n",
    "print(true_50/total_50)\n",
    "print(true_75/total_75)\n",
    "print(true_100/total_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_2 = []\n",
    "for i in range(len(submatching_score)):\n",
    "    score_tmp = submatching_score[i]\n",
    "    score_tmp.insert(0,1)\n",
    "    for j in reversed(range(i)):\n",
    "        score_tmp.insert(0,submatching_score[j][i-j])\n",
    "    score_list_2.append(score_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating top-k accuracies for half matching (1440) images\n",
    "total_1 = 0\n",
    "total_5 = 0\n",
    "total_10 = 0\n",
    "total_25 = 0\n",
    "total_50 = 0\n",
    "total_75 = 0\n",
    "total_100 = 0\n",
    "true_1 = 0\n",
    "true_5 = 0\n",
    "true_10 = 0\n",
    "true_25 = 0\n",
    "true_50 = 0\n",
    "true_75 = 0\n",
    "true_100 = 0\n",
    "\n",
    "for i in range(len(score_list_2)):\n",
    "        true_label = total_labels[i]\n",
    "        top_1_idx = np.argsort(score_list_2[i])[-1:]\n",
    "        top_5_idx = np.argsort(score_list_2[i])[-5:]\n",
    "        top_10_idx = np.argsort(score_list_2[i])[-10:]\n",
    "        top_25_idx = np.argsort(score_list_2[i])[-25:]\n",
    "        top_50_idx = np.argsort(score_list_2[i])[-50:]\n",
    "        top_75_idx = np.argsort(score_list_2[i])[-75:]\n",
    "        top_100_idx = np.argsort(score_list_2[i])[-100:]\n",
    "        total_1 += 1\n",
    "        total_5 += 1\n",
    "        total_10 += 1\n",
    "        total_25 += 1\n",
    "        total_50 += 1\n",
    "        total_75 += 1\n",
    "        total_100 += 1\n",
    "        for index in top_1_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    #print(i)\n",
    "                    true_1 += 1\n",
    "                    break\n",
    "        for index in top_5_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_5 += 1\n",
    "                    break\n",
    "        for index in top_10_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_10 += 1\n",
    "                    break\n",
    "        for index in top_25_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_25 += 1\n",
    "                    break\n",
    "        for index in top_50_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_50 += 1\n",
    "                    break\n",
    "        for index in top_75_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_75 += 1\n",
    "                    break\n",
    "        for index in top_100_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_100 += 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08173076923076923\n",
      "0.5336538461538461\n",
      "0.8894230769230769\n",
      "0.9951923076923077\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(true_1/total_1)\n",
    "print(true_5/total_5)\n",
    "print(true_10/total_10)\n",
    "print(true_25/total_25)\n",
    "print(true_50/total_50)\n",
    "print(true_75/total_75)\n",
    "print(true_100/total_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-831-110ffe31626a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# find the keypoints and descriptors with SIFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mkp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# FLANN parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "orimatching_score = []\n",
    "time = []\n",
    "for i in range(719):\n",
    "    print(i)\n",
    "    start = timeit.timeit()\n",
    "    scores = []\n",
    "    img1 = cv2.imread(total_paths[i],0)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    for j in range(i+1,720):\n",
    "        \n",
    "        img2 = cv2.imread(total_paths[j],0)\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "        \n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)   # or pass empty dictionary\n",
    "        flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "        \n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                good.append([m])\n",
    "\n",
    "        scores.append(len(good)/np.min([len(des1),len(des2)]))\n",
    "        \n",
    "    orimatching_score.append(scores)\n",
    "    end = timeit.timeit()\n",
    "    time.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_4 = []\n",
    "for i in range(len(orimatching_score)):\n",
    "    score_tmp = orimatching_score[i]\n",
    "    score_tmp.insert(0,1)\n",
    "    for j in reversed(range(i)):\n",
    "        score_tmp.insert(0,orimatching_score[j][i-j])\n",
    "    score_list_4.append(score_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating top-k accuracies for original matching (720) images\n",
    "total_1 = 0\n",
    "total_5 = 0\n",
    "total_10 = 0\n",
    "total_25 = 0\n",
    "total_50 = 0\n",
    "total_75 = 0\n",
    "total_100 = 0\n",
    "true_1 = 0\n",
    "true_5 = 0\n",
    "true_10 = 0\n",
    "true_25 = 0\n",
    "true_50 = 0\n",
    "true_75 = 0\n",
    "true_100 = 0\n",
    "\n",
    "for i in range(len(score_list_4)):\n",
    "        true_label = total_labels[i]\n",
    "        top_1_idx = np.argsort(score_list_4[i])[-1:]\n",
    "        top_5_idx = np.argsort(score_list_4[i])[-5:]\n",
    "        top_10_idx = np.argsort(score_list_4[i])[-10:]\n",
    "        top_25_idx = np.argsort(score_list_4[i])[-25:]\n",
    "        top_50_idx = np.argsort(score_list_4[i])[-50:]\n",
    "        top_75_idx = np.argsort(score_list_4[i])[-75:]\n",
    "        top_100_idx = np.argsort(score_list_4[i])[-100:]\n",
    "        total_1 += 1\n",
    "        total_5 += 1\n",
    "        total_10 += 1\n",
    "        total_25 += 1\n",
    "        total_50 += 1\n",
    "        total_75 += 1\n",
    "        total_100 += 1\n",
    "        for index in top_1_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    #print(i)\n",
    "                    true_1 += 1\n",
    "                    break\n",
    "        for index in top_5_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_5 += 1\n",
    "                    break\n",
    "        for index in top_10_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_10 += 1\n",
    "                    break\n",
    "        for index in top_25_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_25 += 1\n",
    "                    break\n",
    "        for index in top_50_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_50 += 1\n",
    "                    break\n",
    "        for index in top_75_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_75 += 1\n",
    "                    break\n",
    "        for index in top_100_idx:\n",
    "            #if index < len(aug_image_labels):\n",
    "                if total_labels[index] == true_label:\n",
    "                    true_100 += 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37337662337662336\n",
      "0.9642857142857143\n",
      "0.9967532467532467\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(true_1/total_1)\n",
    "print(true_5/total_5)\n",
    "print(true_10/total_10)\n",
    "print(true_25/total_25)\n",
    "print(true_50/total_50)\n",
    "print(true_75/total_75)\n",
    "print(true_100/total_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('no_aug_testset.txt', 'w') as f:\n",
    "    for item in total_paths[:308]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2",
   "language": "python",
   "name": "cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
